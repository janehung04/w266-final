{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data\n",
    "**Author:** Jane Hung  \n",
    "**Date:** 1 Mar 2020  \n",
    "**Citations:**  \n",
    "@inproceedings{xu_bert2019,\n",
    "    title = \"BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis\",\n",
    "    author = \"Xu, Hu and Liu, Bing and Shu, Lei and Yu, Philip S.\",\n",
    "    booktitle = \"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics\",\n",
    "    year = \"2019\",\n",
    "}  \n",
    "https://drive.google.com/file/d/1NGH5bqzEx6aDlYJ7O3hepZF4i_p4iMR8/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    f = open(filename,'r')\n",
    "    data = json.loads(f.read())\n",
    "    print('\\n',filename)\n",
    "    pprint.pprint(dict(list(data.items())[:1]))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ../data/hu-data/ae/laptop/train.json\n",
      "{'0': {'label': ['B',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'B',\n",
      "                 'I',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O'],\n",
      "       'sentence': ['Keyboard',\n",
      "                    'is',\n",
      "                    'great',\n",
      "                    'but',\n",
      "                    'primary',\n",
      "                    'and',\n",
      "                    'secondary',\n",
      "                    'control',\n",
      "                    'buttons',\n",
      "                    'could',\n",
      "                    'be',\n",
      "                    'more',\n",
      "                    'durable',\n",
      "                    '.']}}\n",
      "\n",
      " ../data/hu-data/ae/rest/train.json\n",
      "{'0': {'label': ['O', 'O', 'O', 'B'],\n",
      "       'sentence': ['I', 'LOVE', 'their', 'Thai']}}\n",
      "\n",
      " ../data/hu-data/asc/laptop/train.json\n",
      "{'327_0': {'id': '327_0',\n",
      "           'polarity': 'positive',\n",
      "           'sentence': 'Also it is very good for college students who just '\n",
      "                       'need a reliable, easy to use computer.',\n",
      "           'term': 'use'}}\n",
      "\n",
      " ../data/hu-data/asc/rest/train.json\n",
      "{'1592_0': {'id': '1592_0',\n",
      "            'polarity': 'positive',\n",
      "            'sentence': 'Our server was very helpful and friendly.',\n",
      "            'term': 'server'}}\n"
     ]
    }
   ],
   "source": [
    "ae_laptop_train = read_json('../data/hu-data/ae/laptop/train.json')\n",
    "ae_rest_train = read_json('../data/hu-data/ae/rest/train.json')\n",
    "\n",
    "\n",
    "asc_laptop_train = read_json('../data/hu-data/asc/laptop/train.json')\n",
    "asc_rest_train = read_json('../data/hu-data/asc/rest/train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ../data/hu-data/ae/laptop/dev.json\n",
      "{'0': {'label': ['O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O'],\n",
      "       'sentence': ['I',\n",
      "                    'have',\n",
      "                    'had',\n",
      "                    'this',\n",
      "                    'laptop',\n",
      "                    'for',\n",
      "                    'a',\n",
      "                    'few',\n",
      "                    'months',\n",
      "                    'now',\n",
      "                    'and',\n",
      "                    'i',\n",
      "                    'would',\n",
      "                    'say',\n",
      "                    'im',\n",
      "                    'pretty',\n",
      "                    'satisfied',\n",
      "                    '.']}}\n",
      "\n",
      " ../data/hu-data/ae/rest/dev.json\n",
      "{'0': {'label': ['O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'B',\n",
      "                 'I',\n",
      "                 'I',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O'],\n",
      "       'sentence': ['In',\n",
      "                    'the',\n",
      "                    'summer',\n",
      "                    'months',\n",
      "                    ',',\n",
      "                    'the',\n",
      "                    'back',\n",
      "                    'garden',\n",
      "                    'area',\n",
      "                    'is',\n",
      "                    'really',\n",
      "                    'nice',\n",
      "                    '.']}}\n",
      "\n",
      " ../data/hu-data/asc/laptop/dev.json\n",
      "{'1113_0': {'id': '1113_0',\n",
      "            'polarity': 'negative',\n",
      "            'sentence': 'Not even safe mode boots.',\n",
      "            'term': 'safe mode'}}\n",
      "\n",
      " ../data/hu-data/asc/rest/dev.json\n",
      "{'2516_0': {'id': '2516_0',\n",
      "            'polarity': 'positive',\n",
      "            'sentence': \"This is the only Thai place I go too in NYC, it's \"\n",
      "                        'wonderful, and live relaxed Jazz on certain nights.',\n",
      "            'term': 'Jazz'}}\n"
     ]
    }
   ],
   "source": [
    "ae_laptop_dev  = read_json('../data/hu-data/ae/laptop/dev.json')\n",
    "ae_rest_dev = read_json('../data/hu-data/ae/rest/dev.json')\n",
    "\n",
    "\n",
    "asc_laptop_dev = read_json('../data/hu-data/asc/laptop/dev.json')\n",
    "asc_rest_dev = read_json('../data/hu-data/asc/rest/dev.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ../data/hu-data/ae/laptop/test.json\n",
      "{'0': {'label': ['B',\n",
      "                 'I',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O'],\n",
      "       'sentence': ['Boot',\n",
      "                    'time',\n",
      "                    'is',\n",
      "                    'super',\n",
      "                    'fast',\n",
      "                    ',',\n",
      "                    'around',\n",
      "                    'anywhere',\n",
      "                    'from',\n",
      "                    '35',\n",
      "                    'seconds',\n",
      "                    'to',\n",
      "                    '1',\n",
      "                    'minute',\n",
      "                    '.']}}\n",
      "\n",
      " ../data/hu-data/ae/rest/test.json\n",
      "{'0': {'label': ['O', 'O'], 'sentence': ['Yum', '!']}}\n",
      "\n",
      " ../data/hu-data/asc/laptop/test.json\n",
      "{'718:1_0': {'id': '718:1_0',\n",
      "             'polarity': 'positive',\n",
      "             'sentence': 'the retina display display make pictures i took '\n",
      "                         'years ago jaw dropping.',\n",
      "             'term': 'retina display display'}}\n",
      "\n",
      " ../data/hu-data/asc/rest/test.json\n",
      "{'11359619#487952#7_1': {'id': '11359619#487952#7_1',\n",
      "                         'polarity': 'positive',\n",
      "                         'sentence': 'for an appetizer, their calamari is a '\n",
      "                                     'winner.',\n",
      "                         'term': 'calamari'}}\n"
     ]
    }
   ],
   "source": [
    "ae_laptop_test  = read_json('../data/hu-data/ae/laptop/test.json')\n",
    "ae_rest_test = read_json('../data/hu-data/ae/rest/test.json')\n",
    "\n",
    "\n",
    "asc_laptop_test = read_json('../data/hu-data/asc/laptop/test.json')\n",
    "asc_rest_test = read_json('../data/hu-data/asc/rest/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[B, O, O, O, O, O, O, B, I, O, O, O, O, O]</td>\n",
       "      <td>[Keyboard, is, great, but, primary, and, secon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[I, bought, this, laptop, about, a, month, ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[I, am, however, pleased, that, it, is, still,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[I, went, to, my, local, Best, Buy, looking, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[The, Apple, MC371LL/, A, 2.4Ghz, 15.4-, inch,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  \\\n",
       "0         [B, O, O, O, O, O, O, B, I, O, O, O, O, O]   \n",
       "1         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2               [O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                            sentence  \n",
       "0  [Keyboard, is, great, but, primary, and, secon...  \n",
       "1  [I, bought, this, laptop, about, a, month, ago...  \n",
       "2  [I, am, however, pleased, that, it, is, still,...  \n",
       "3  [I, went, to, my, local, Best, Buy, looking, f...  \n",
       "4  [The, Apple, MC371LL/, A, 2.4Ghz, 15.4-, inch,...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_laptop_train_df = pd.DataFrame.from_dict(ae_laptop_train,orient='index')\n",
    "ae_laptop_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[I, have, had, this, laptop, for, a, few, mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, B, I, O, O, O, O, O, O, B, O, O, ...</td>\n",
       "      <td>[Additional, caveat, :, the, base, installatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, B, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "      <td>[it, is, of, high, quality, ,, has, a, killer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, B, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[The, screen, gets, smeary, and, dusty, very, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[I, previously, owned, an, HP, desktop, and, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, B, I, O, O, O, O, O, O, B, O, O, ...   \n",
       "2  [O, O, O, O, B, O, O, O, O, B, O, O, O, O, O, ...   \n",
       "3         [O, B, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                            sentence  \n",
       "0  [I, have, had, this, laptop, for, a, few, mont...  \n",
       "1  [Additional, caveat, :, the, base, installatio...  \n",
       "2  [it, is, of, high, quality, ,, has, a, killer,...  \n",
       "3  [The, screen, gets, smeary, and, dusty, very, ...  \n",
       "4  [I, previously, owned, an, HP, desktop, and, a...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_laptop_dev_df = pd.DataFrame.from_dict(ae_laptop_dev,orient='index')\n",
    "ae_laptop_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[B, I, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[Boot, time, is, super, fast, ,, around, anywh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[B, I, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[tech, support, would, not, fix, the, problem,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>[but, in, resume, this, computer, rocks, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[B, I, O, O, O]</td>\n",
       "      <td>[Set, up, was, easy, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, B, I, O, B, I, O]</td>\n",
       "      <td>[Did, not, enjoy, the, new, Windows, 8, and, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  \\\n",
       "0      [B, I, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1  [B, I, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2                              [O, O, O, O, O, O, O]   \n",
       "3                                    [B, I, O, O, O]   \n",
       "4                  [O, O, O, O, O, B, I, O, B, I, O]   \n",
       "\n",
       "                                            sentence  \n",
       "0  [Boot, time, is, super, fast, ,, around, anywh...  \n",
       "1  [tech, support, would, not, fix, the, problem,...  \n",
       "2        [but, in, resume, this, computer, rocks, !]  \n",
       "3                            [Set, up, was, easy, .]  \n",
       "4  [Did, not, enjoy, the, new, Windows, 8, and, t...  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_laptop_test_df = pd.DataFrame.from_dict(ae_laptop_test,orient='index')\n",
    "ae_laptop_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>term</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327_0</th>\n",
       "      <td>positive</td>\n",
       "      <td>use</td>\n",
       "      <td>327_0</td>\n",
       "      <td>Also it is very good for college students who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077_0</th>\n",
       "      <td>positive</td>\n",
       "      <td>noise</td>\n",
       "      <td>3077_0</td>\n",
       "      <td>For those that care about noise this thing doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592_1</th>\n",
       "      <td>positive</td>\n",
       "      <td>force</td>\n",
       "      <td>1592_1</td>\n",
       "      <td>Enjoy that Toshib force and durability unparal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>expense</td>\n",
       "      <td>329_0</td>\n",
       "      <td>I know that everyone thinks Macs are overprice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>word processor</td>\n",
       "      <td>1184_0</td>\n",
       "      <td>) And printing from either word processor is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity            term      id  \\\n",
       "327_0   positive             use   327_0   \n",
       "3077_0  positive           noise  3077_0   \n",
       "1592_1  positive           force  1592_1   \n",
       "329_0   negative         expense   329_0   \n",
       "1184_0  negative  word processor  1184_0   \n",
       "\n",
       "                                                 sentence  \n",
       "327_0   Also it is very good for college students who ...  \n",
       "3077_0  For those that care about noise this thing doe...  \n",
       "1592_1  Enjoy that Toshib force and durability unparal...  \n",
       "329_0   I know that everyone thinks Macs are overprice...  \n",
       "1184_0  ) And printing from either word processor is a...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_laptop_train_df = pd.DataFrame.from_dict(asc_laptop_train,orient='index')\n",
    "asc_laptop_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>term</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1113_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>safe mode</td>\n",
       "      <td>1113_0</td>\n",
       "      <td>Not even safe mode boots.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595_0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>2595_0</td>\n",
       "      <td>Keyboard was also very nice and had a solid feel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>1039_0</td>\n",
       "      <td>Keyboard is plastic and spongey feeling.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315_0</th>\n",
       "      <td>positive</td>\n",
       "      <td>quality</td>\n",
       "      <td>315_0</td>\n",
       "      <td>I would recommend this laptop to anyone lookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>screen</td>\n",
       "      <td>1284_0</td>\n",
       "      <td>Thus, when you carry it at a slanted angle, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity       term      id  \\\n",
       "1113_0  negative  safe mode  1113_0   \n",
       "2595_0  positive   Keyboard  2595_0   \n",
       "1039_0  negative   Keyboard  1039_0   \n",
       "315_0   positive    quality   315_0   \n",
       "1284_0  negative     screen  1284_0   \n",
       "\n",
       "                                                 sentence  \n",
       "1113_0                          Not even safe mode boots.  \n",
       "2595_0  Keyboard was also very nice and had a solid feel.  \n",
       "1039_0           Keyboard is plastic and spongey feeling.  \n",
       "315_0   I would recommend this laptop to anyone lookin...  \n",
       "1284_0  Thus, when you carry it at a slanted angle, th...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_laptop_dev_df = pd.DataFrame.from_dict(asc_laptop_dev,orient='index')\n",
    "asc_laptop_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>term</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>718:1_0</th>\n",
       "      <td>positive</td>\n",
       "      <td>retina display display</td>\n",
       "      <td>718:1_0</td>\n",
       "      <td>the retina display display make pictures i too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217:1_1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>CD/DVD drive</td>\n",
       "      <td>217:1_1</td>\n",
       "      <td>Needs a CD/DVD drive and a bigger power switch.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217:1_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>power switch</td>\n",
       "      <td>217:1_0</td>\n",
       "      <td>Needs a CD/DVD drive and a bigger power switch.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044:1_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>battery</td>\n",
       "      <td>1044:1_0</td>\n",
       "      <td>The battery is not as shown in the product pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040:1_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>1040:1_0</td>\n",
       "      <td>It feels cheap, the keyboard is not very sensi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          polarity                    term        id  \\\n",
       "718:1_0   positive  retina display display   718:1_0   \n",
       "217:1_1    neutral            CD/DVD drive   217:1_1   \n",
       "217:1_0   negative            power switch   217:1_0   \n",
       "1044:1_0  negative                 battery  1044:1_0   \n",
       "1040:1_0  negative                keyboard  1040:1_0   \n",
       "\n",
       "                                                   sentence  \n",
       "718:1_0   the retina display display make pictures i too...  \n",
       "217:1_1     Needs a CD/DVD drive and a bigger power switch.  \n",
       "217:1_0     Needs a CD/DVD drive and a bigger power switch.  \n",
       "1044:1_0  The battery is not as shown in the product pho...  \n",
       "1040:1_0  It feels cheap, the keyboard is not very sensi...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_laptop_test_df = pd.DataFrame.from_dict(asc_laptop_test,orient='index')\n",
    "asc_laptop_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with MultiNomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2895"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences = [val['sentence'] for key, val in ae_laptop_train.items()]\n",
    "batch_sentences.extend([val['sentence'] for key, val in ae_laptop_dev.items()])\n",
    "batch_sentences.extend([val['sentence'] for key, val in ae_laptop_test.items()])\n",
    "len(ae_laptop_train)\n",
    "len(ae_laptop_dev)\n",
    "len(ae_laptop_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_label = [val['label'] for key, val in ae_laptop_train.items()]\n",
    "batch_label.extend([val['label'] for key, val in ae_laptop_dev.items()])\n",
    "batch_label.extend([val['label'] for key, val in ae_laptop_test.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word, pos, ner):\n",
    "    \"\"\"\n",
    "    Convert a word into a word token and add supplied NER and POS labels. Note that the word can be  \n",
    "    tokenized to two or more tokens. Correspondingly, we add - for now - custom 'X' tokens to the labels in order to \n",
    "    maintain the 1:1 mappings between word tokens and labels.\n",
    "    \n",
    "    arguments: word, pos label, ner label\n",
    "    returns: dictionary with tokens and labels\n",
    "    \"\"\"\n",
    "    # the dataset contains various '\"\"\"' combinations which we choose to truncate to '\"', etc. \n",
    "    if word == '\"\"\"\"':\n",
    "        word = '\"'\n",
    "    elif word == '``':\n",
    "        word = '`'\n",
    "        \n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
    "    \n",
    "    addDict = dict()\n",
    "    \n",
    "    addDict['wordToken'] = tokens\n",
    "    addDict['posToken'] = [pos] + ['posX'] * (tokenLength - 1)\n",
    "    addDict['nerToken'] = [ner] + ['nerX'] * (tokenLength - 1)\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "    \n",
    "    \n",
    "    return addDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists for sentences, tokens, labels, etc.  \n",
    "sentenceList = []\n",
    "sentenceTokenList = []\n",
    "posTokenList = []\n",
    "nerTokenList = []\n",
    "sentLengthList = []\n",
    "\n",
    "# lists for BERT input\n",
    "bertSentenceIDs = []\n",
    "bertMasks = []\n",
    "bertSequenceIDs = []\n",
    "\n",
    "sentence = ''\n",
    "\n",
    "# always start with [CLS] tokens\n",
    "sentenceTokens = ['[CLS]']\n",
    "posTokens = ['[posCLS]']\n",
    "nerTokens = ['[nerCLS]']\n",
    "\n",
    "for sentence,label in zip(batch_sentences,batch_label):\n",
    "    for ind,token in enumerate(sentence):\n",
    "        word, pos, ner = token, nltk.pos_tag(token),label[ind]\n",
    "\n",
    "        # if new sentence starts\n",
    "        if (ind == 0):            \n",
    "\n",
    "            sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "            sentLengthList.append(sentenceLength)\n",
    "\n",
    "            # Create space for at least a final '[SEP]' token\n",
    "            if sentenceLength >= max_length - 1: \n",
    "                sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "                posTokens = posTokens[:max_length - 2]\n",
    "                nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "            # add a ['SEP'] token and padding\n",
    "\n",
    "            sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "\n",
    "            posTokens += ['[posSEP]'] + ['[posPAD]'] * (max_length - 1 - len(posTokens) )\n",
    "            nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "\n",
    "            sentenceList.append(sentence)\n",
    "\n",
    "            sentenceTokenList.append(sentenceTokens)\n",
    "\n",
    "            bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "            bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "            bertSequenceIDs.append([0] * (max_length))\n",
    "\n",
    "            posTokenList.append(posTokens)\n",
    "            nerTokenList.append(nerTokens)\n",
    "\n",
    "            sentence = ''\n",
    "            sentenceTokens = ['[CLS]']\n",
    "            posTokens = ['[posCLS]']\n",
    "            nerTokens = ['[nerCLS]']\n",
    "\n",
    "            sentence += ' ' + word\n",
    "\n",
    "        addDict = addWord(word, pos, ner)\n",
    "\n",
    "        sentenceTokens += addDict['wordToken']\n",
    "        posTokens += addDict['posToken']\n",
    "        nerTokens += addDict['nerToken']\n",
    "\n",
    "# The first two list elements need to be removed. 1st line in file is a-typical, and 2nd line does not end a sentence   \n",
    "# sentLengthList = sentLengthList[2:]\n",
    "# sentenceTokenList = sentenceTokenList[2:]\n",
    "# bertSentenceIDs = bertSentenceIDs[2:]\n",
    "# bertMasks = bertMasks[2:]\n",
    "# bertSequenceIDs = bertSequenceIDs[2:]\n",
    "# posTokenList = posTokenList[2:]\n",
    "# nerTokenList = nerTokenList[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Key', '##board', 'is', 'great', 'but', 'primary', 'and', 'secondary', 'control', 'buttons', 'could', 'be', 'more', 'du', '##rable', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(sentenceTokenList[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[nerCLS]', 'B', 'nerX', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'nerX', 'O', '[nerSEP]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]']\n"
     ]
    }
   ],
   "source": [
    "print(nerTokenList[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(bertMasks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(bertSequenceIDs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   2.,  30.,  38.,  90., 137., 124., 173., 175., 154.,\n",
       "        174., 203., 197., 192., 173., 193., 138., 138., 147., 109., 118.,\n",
       "         88., 101.,  92.,  88.,  89.,  84.,  50.,  48.,  43.,  46.,   0.,\n",
       "         48.,  37.,  29.,  30.,  22.,  17.,  20.,  26.,  20.,  10.,  13.,\n",
       "          9.,  19.,  10.,  11.,   6.,   8.,   4.,   5.,   7.,   3.,   5.,\n",
       "          6.,   5.,   3.,   2.,   3.,   2.,   2.,   4.,   2.,   1.,   0.,\n",
       "          1.,   0.,   0.,   1.,   3.,   0.,   2.,   1.,   0.,   3.,   1.,\n",
       "          1.,   1.,   1.,   1.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "          2.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   1.]),\n",
       " array([ 1.        ,  1.96938776,  2.93877551,  3.90816327,  4.87755102,\n",
       "         5.84693878,  6.81632653,  7.78571429,  8.75510204,  9.7244898 ,\n",
       "        10.69387755, 11.66326531, 12.63265306, 13.60204082, 14.57142857,\n",
       "        15.54081633, 16.51020408, 17.47959184, 18.44897959, 19.41836735,\n",
       "        20.3877551 , 21.35714286, 22.32653061, 23.29591837, 24.26530612,\n",
       "        25.23469388, 26.20408163, 27.17346939, 28.14285714, 29.1122449 ,\n",
       "        30.08163265, 31.05102041, 32.02040816, 32.98979592, 33.95918367,\n",
       "        34.92857143, 35.89795918, 36.86734694, 37.83673469, 38.80612245,\n",
       "        39.7755102 , 40.74489796, 41.71428571, 42.68367347, 43.65306122,\n",
       "        44.62244898, 45.59183673, 46.56122449, 47.53061224, 48.5       ,\n",
       "        49.46938776, 50.43877551, 51.40816327, 52.37755102, 53.34693878,\n",
       "        54.31632653, 55.28571429, 56.25510204, 57.2244898 , 58.19387755,\n",
       "        59.16326531, 60.13265306, 61.10204082, 62.07142857, 63.04081633,\n",
       "        64.01020408, 64.97959184, 65.94897959, 66.91836735, 67.8877551 ,\n",
       "        68.85714286, 69.82653061, 70.79591837, 71.76530612, 72.73469388,\n",
       "        73.70408163, 74.67346939, 75.64285714, 76.6122449 , 77.58163265,\n",
       "        78.55102041, 79.52040816, 80.48979592, 81.45918367, 82.42857143,\n",
       "        83.39795918, 84.36734694, 85.33673469, 86.30612245, 87.2755102 ,\n",
       "        88.24489796, 89.21428571, 90.18367347, 91.15306122, 92.12244898,\n",
       "        93.09183673, 94.06122449, 95.03061224, 96.        ]),\n",
       " <BarContainer object of 98 artists>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentenceLengths= [l for l in sentLengthList]\n",
    "\n",
    "plt.hist(np.array(sentenceLengths), bins=(max_length-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DON'T USE THE BELOW BERT AND NB STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3045, 97), dtype=int32, numpy=\n",
       "array([[  101,  7443,  4015, ...,     0,     0,     0],\n",
       "       [  101,   146,  3306, ...,     0,     0,     0],\n",
       "       [  101,   146,  1821, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1198, 11824, ...,     0,     0,     0],\n",
       "       [  101,  1422,  1488, ...,     0,     0,     0],\n",
       "       [  101,  1188,  1110, ...,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3045, 97), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3045, 97), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences, padding=True, is_split_into_words=True,return_tensors='tf',max_length=50)\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60156, 1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([3173, 1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate encoder values into train/test data\n",
    "# TODO update encoder with Test data as well\n",
    "train_sentences = encoded_inputs['input_ids'][:2895]\n",
    "test_sentences = encoded_inputs['input_ids'][-150:]\n",
    "\n",
    "# build boolean mask to flatten all sentences into tokens\n",
    "train_mask = encoded_inputs['attention_mask'][:2895]\n",
    "test_mask = encoded_inputs['attention_mask'][-150:]\n",
    "\n",
    "train_sentences = tf.boolean_mask(train_sentences, train_mask)\n",
    "train_sentences = tf.reshape(train_sentences,[-1,1])\n",
    "train_sentences.shape\n",
    "\n",
    "test_sentences = tf.boolean_mask(test_sentences, test_mask)\n",
    "test_sentences = tf.reshape(test_sentences,[-1,1])\n",
    "test_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1330,\n",
       "  '[CLS] The # 1 reason that is always repeated. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'),\n",
       " (2186,\n",
       "  '[CLS] The materials that came with the computer did not include the right # anywhere. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = encoded_inputs['input_ids'][:2895]\n",
    "# train_sentences = tf.boolean_mask(train_sentences, train_mask)\n",
    "# train_sentences[:18]\n",
    "[(ind,tokenizer.decode(sent)) for ind,sent in enumerate(train_sentences) if '#' in tokenizer.decode(sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(54747,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2906,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_labels = np.array([token for key, val in ae_laptop_train.items() for token in val['label']])\n",
    "# test_labels = np.array([token for key, val in ae_laptop_dev.items() for token in val['label']])\n",
    "label_list = ['I','O','B']\n",
    "\n",
    "def format_labels(data_dict):\n",
    "    # get the label for each sample\n",
    "    convert_labels = [val['label'] for key, val in data_dict.items() ]\n",
    "\n",
    "    # convert every token to a numerical label\n",
    "    convert_labels = [[label_list.index(token) for token in label] for label in convert_labels]\n",
    "    \n",
    "    # add the class and sep tokens\n",
    "    convert_labels_num = []\n",
    "    for label in convert_labels:\n",
    "        # class token\n",
    "        sp_token = [1]\n",
    "        sp_token.extend(label)\n",
    "        # sep token\n",
    "        sp_token.append(1)\n",
    "        convert_labels_num.extend(sp_token)\n",
    "    \n",
    "#     print(convert_labels_num)\n",
    "    convert_labels_num = np.array(convert_labels_num)\n",
    "    return(convert_labels_num.reshape(-1))\n",
    "train_labels = format_labels(ae_laptop_train)\n",
    "test_labels = format_labels(ae_laptop_dev)\n",
    "train_labels\n",
    "train_labels.shape\n",
    "test_labels\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2895, 54747]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-90d6dfe9a16d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \"\"\"\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    257\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2895, 54747]"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB().fit(train_sentences, train_labels)\n",
    "y_pred = nb.predict(test_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_labels,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Counter([label_list[pred] for pred in y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    in_x = tf.keras.layers.Input=(shape=(5,34), name='in_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "model.fit(train_dataset, epochs=2, steps_per_epoch=115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_outputs = bert_layer(encoded_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(256,activation='relu',name='dense')(bert_outputs)\n",
    "dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "dense = tf.keras.layers.Dense(128,activation='relu',name='dense')(dense)\n",
    "dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "pred = tf.keras.layers.Dense(3,activation='softmax',name='ner')(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.argmax(pred, axis=2)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"B\",\"I\",\"O\"\n",
    "]\n",
    "[label_list[pred] for predict in predictions for pred in predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asc\n",
    "batch_sentences = [val['sentence'] for key,val in asc_laptop_train.items()]\n",
    "batch_sentences = batch_sentences[:5]\n",
    "batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_outputs = bert_layer(batch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_outputs)\n",
    "\n",
    "dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "\n",
    "pred = tf.keras.layers.Dense(21, activation='softmax', name='ner')(dense)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=batch, outputs=pred)\n",
    "\n",
    "model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                      custom_acc_orig_non_other_tokens])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with AE baseline - NN+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_ae(tokenized_sentence):\n",
    "    \"\"\"\n",
    "    Tag sentences using POS tagger and identify consecutive nouns as entities\n",
    "    \"\"\"\n",
    "    pos_sent = tokenized_sentence.apply(lambda sent:nltk.pos_tag(sent,tagset='universal'))\n",
    "    \n",
    "    \n",
    "    # tag with IOB terminology\n",
    "    ae_tag = lambda sent:['O' if token[1] != 'NOUN' \n",
    "                          else 'B' if ((token[1]=='NOUN') & ((sent[ind-1][1]!='NOUN') | (ind==0))) \n",
    "                          else 'I' for ind,token in enumerate(sent)]\n",
    "\n",
    "    return(pos_sent.apply(ae_tag))\n",
    "\n",
    "# since the POS tagger is based on the words themselves and not context.\n",
    "ae_laptop_dev_df['predictions'] = pos_ae(ae_laptop_dev_df['sentence'])\n",
    "ae_laptop_dev_df.head()\n",
    "\n",
    "def convert_int(tagged_tokens):\n",
    "    \"\"\"\n",
    "    Convert B,I,O tags to integers\n",
    "    \"\"\"\n",
    "    return(tagged_tokens.apply(lambda sent: [0 if token=='O' else 1 if token=='B' else 2 for token in sent]))\n",
    "\n",
    "convert_int(ae_laptop_dev_df['predictions'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AE Regex Parser - business rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a more sophisticated method for chunking\n",
    "def regex_parser(tokenized_sentence,verbose=False):\n",
    "    \"\"\"\n",
    "    Use a Regex Parser to provide some context around noun phrases\n",
    "    \"\"\"\n",
    "    pos_sent = nltk.pos_tag(tokenized_sentence)\n",
    "#     print(pos_sent)\n",
    "#     grammar = r\"\"\"\n",
    "#       NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "#           {<NNP>+}                # chunk sequences of proper nouns\n",
    "#     \"\"\"\n",
    "    \n",
    "    # Update Grammar Regex to include prepositional phrases ala Semeval annotation guidelines\n",
    "    grammar = r\"\"\"\n",
    "    NP: {<NN><IN><DT><NN|NNP>+}\n",
    "        {<NNP><NN>}\n",
    "        {<NNP>+}\n",
    "        {<NN>+}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "    tree = cp.parse(pos_sent)\n",
    "    \n",
    "    if verbose: print(tree)\n",
    "    \n",
    "    iob = [el[2][0] for el in nltk.chunk.util.tree2conlltags(tree)]\n",
    "    \n",
    "    return(iob)\n",
    "\n",
    "# print example\n",
    "ae_laptop_train['15']['sentence']\n",
    "regex_parser(ae_laptop_train['15']['sentence'])\n",
    "\n",
    "print(['cover','for','the','DVD','drive'])\n",
    "regex_parser(['cover','for','the','DVD','drive'])\n",
    "\n",
    "ae_laptop_dev_df.iloc[4]['sentence']\n",
    "regex_parser(ae_laptop_dev_df.iloc[4]['sentence'],verbose=True)\n",
    "\n",
    "# since the POS tagger is based on the words themselves and not context.\n",
    "ae_laptop_dev_df['predictions_1'] = ae_laptop_dev_df['sentence'].apply(lambda x: regex_parser(x))\n",
    "ae_laptop_dev_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AE evaluation - CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haven't seen many papers using CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only using 0,1 because there aren't many very large token phrases\n",
    "log_loss(convert_int(pd.DataFrame(ae_laptop_dev_df.iloc[0]['label'])),convert_int(pd.DataFrame(ae_laptop_dev_df.iloc[0]['predictions'])),labels=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AE evaluation - SemEval14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/  \n",
    "- partial boundary match over the surface string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to explore how we want to move forward with all sentences rather than just 1.\n",
    "# Should try to implement the SemEval14 evaluation criteria bc this is best practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO amend this tree structure for all predictions as well\n",
    "print('\\nGold Standard:')\n",
    "# tag every sentence with the pos\n",
    "gold_tree = ae_laptop_dev_df['sentence'].apply(lambda x: nltk.pos_tag(x))\n",
    "print(gold_tree)\n",
    "iob_gold_tree = [nltk.Tree('S',\n",
    "                           [(el[0], el[1], ae_laptop_dev_df.iloc[tree_ind]['label'][ind])\n",
    "                            if ae_laptop_dev_df.iloc[tree_ind]['label'][ind]=='O'\n",
    "                            else (el[0], el[1], ae_laptop_dev_df.iloc[tree_ind]['label'][ind] + '-NP')\n",
    "                            for ind,el in enumerate(tree)])\n",
    "                for tree_ind, tree in enumerate(gold_tree)]\n",
    "ae_laptop_dev_df['iob_gold_tree'] = iob_gold_tree\n",
    "ae_laptop_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sentence_lst, predictions_lst):\n",
    "    \"\"\"\n",
    "    Reformat the IOB structure to get the actual entities from the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # for every sentence, iterate through\n",
    "    all_entities = []\n",
    "    for sample in range(len(predictions_lst)):\n",
    "    \n",
    "        # get indices where entities are identified\n",
    "        predictions = np.array(predictions_lst[sample])\n",
    "        ind = (predictions == 'B') | (predictions == 'I')\n",
    "        \n",
    "        # create list of numerical indices and boolean indices. ex. [(4, True), (10, True), (11, True), (15, True)]\n",
    "        ind_tuple = [num_ind for num_ind in list(enumerate(ind)) if num_ind[1]==True]\n",
    "        \n",
    "        # get the sentence of interest. identify what these entities are\n",
    "        sentence = np.array(sentence_lst[sample])\n",
    "\n",
    "        # group the phrases together\n",
    "        entities = []\n",
    "        for subset,num_ind_tuple in zip(sentence[ind], ind_tuple): # [('price', (4, True)), ('netbook', (10, True)), ('*', (11, True)), ('machine', (15, True))]\n",
    "            # put the B in entities\n",
    "            if predictions[num_ind_tuple[0]][0] == 'B':\n",
    "                entities.append([subset])\n",
    "            # if the tag is I, add to the last item of the list\n",
    "            elif predictions[num_ind_tuple[0]][0] == 'I':\n",
    "                last_entry = entities.pop()\n",
    "                last_entry.append(subset)\n",
    "                entities.append(last_entry)\n",
    "            # there should not be any 'O' indices here\n",
    "            else:\n",
    "                print('Error')\n",
    "        all_entities.append(entities)\n",
    "    return(all_entities)\n",
    "\n",
    "prediction_entities = get_entities(ae_laptop_dev_df.sentence,ae_laptop_dev_df.predictions)\n",
    "gold_entities = get_entities(ae_laptop_dev_df.sentence,ae_laptop_dev_df.label)\n",
    "prediction_entities[:5]\n",
    "gold_entities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ae_eval_features(gold_entities,prediction_entities,verbose=False):\n",
    "    # TODO need to generalize and do for all samples\n",
    "    # TODO may later need to update these calculations to encompass sentence location.\n",
    "    y_true_df = pd.DataFrame([[ind,sub_el] for ind,el in enumerate(gold_entities) for sub_el in el], columns=['sample_index','entity'])\n",
    "    y_pred_df = pd.DataFrame([[ind,sub_el] for ind,el in enumerate(prediction_entities) for sub_el in el], columns=['sample_index','entity'])\n",
    "    print('True')\n",
    "    display(y_true_df.head())\n",
    "    print('Pred')\n",
    "    display(y_pred_df.head())\n",
    "\n",
    "    cor = 0\n",
    "    inc = 0\n",
    "    par = 0\n",
    "    mis = 0\n",
    "    spu = 0\n",
    "\n",
    "    for el in range(len(gold_entities)):\n",
    "        if verbose:\n",
    "            print('\\n',el)\n",
    "        true_subset = y_true_df[y_true_df.sample_index == el]\n",
    "        pred_subset = y_pred_df[y_pred_df.sample_index == el]\n",
    "        true_entities = set(true_subset.entity.apply(lambda x: '_'.join(x)))\n",
    "        pred_entities = set(pred_subset.entity.apply(lambda x: '_'.join(x)))\n",
    "        if verbose:\n",
    "            print('True')\n",
    "            print(true_entities)\n",
    "            print('Pred')\n",
    "            print(pred_entities)\n",
    "\n",
    "        # get correct\n",
    "        cor_entities = true_entities & pred_entities\n",
    "        if verbose:\n",
    "            print(f'Correct entities: {cor_entities}')\n",
    "        cor += len(cor_entities)\n",
    "        true_entities = true_entities - cor_entities\n",
    "        pred_entities = pred_entities - cor_entities\n",
    "\n",
    "        # get partial and missed\n",
    "        for true in true_entities:\n",
    "            # Take into account if the prediction contains a portion of the correct and if correct contains a portion of the prediction\n",
    "            par_entities = set([pred for pred in pred_entities if (true in pred) | (pred in true)])\n",
    "            if len(par_entities) != 0:\n",
    "                if verbose:\n",
    "                    print(f'Partial entities: {set([true])}')\n",
    "                par += len(par_entities)\n",
    "                true_entities = true_entities - set([true])\n",
    "                pred_entities = pred_entities - par_entities\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f'Missed entities: {set([true])}')\n",
    "                mis += 1\n",
    "                true_entities = true_entities - set([true])\n",
    "\n",
    "        if len(true_entities) == 0:\n",
    "            if verbose:\n",
    "                print(f'Spurious entities: {pred_entities}')\n",
    "            spu += len(pred_entities)\n",
    "        else:\n",
    "            print('Error')\n",
    "\n",
    "    print(f'\\nCorrect: {cor}')\n",
    "    print(f'Partial: {par}')\n",
    "    print(f'Missed: {mis}')\n",
    "    print(f'Spurious: {spu}')\n",
    "    return(cor,par,mis,spu,inc)\n",
    "cor,par,mis,spu,inc = get_ae_eval_features(gold_entities,prediction_entities,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ae_eval(sentence_lst, y_true, y_pred,verbose=False):\n",
    "    \"\"\"\n",
    "    Get entity recognition evaluations accoridng to the partial match SemEval strategy\n",
    "    \"\"\"\n",
    "    prediction_entities = get_entities(sentence_lst,y_pred)\n",
    "    gold_entities = get_entities(sentence_lst,y_true)\n",
    "    \n",
    "    cor,par,mis,spu,inc = get_ae_eval_features(gold_entities,prediction_entities,verbose=verbose)\n",
    "    \n",
    "    pos_eval = cor + inc + par + mis\n",
    "    act_eval = cor + inc + par + spu\n",
    "\n",
    "    precision = (cor + .5 * par) / act_eval\n",
    "    recall = (cor + .5 * par) / pos_eval\n",
    "    f1 = ( 2* precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(f'\\nPrecision: \\t{precision}')\n",
    "    print(f'Recall: \\t{recall}')\n",
    "    print(f'F1-Score: \\t{f1}')\n",
    "    return(precision, recall, f1)\n",
    "\n",
    "get_ae_eval(ae_laptop_dev_df.sentence,ae_laptop_dev_df.label,ae_laptop_dev_df.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ae_eval(ae_laptop_dev_df.sentence,ae_laptop_dev_df.label,ae_laptop_dev_df.predictions_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AE evaluation - ChunkScore\n",
    "https://stackoverflow.com/questions/17325554/difference-between-iob-accuracy-and-precision  \n",
    "Somehow, this is not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get ChunkScore to work\n",
    "tokenized_sentence = ae_laptop_train['15']['sentence']\n",
    "pos_sent = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "##\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "tree = cp.parse(pos_sent)\n",
    "\n",
    "iob = [el[2][0] for el in nltk.chunk.util.tree2conlltags(tree)]\n",
    "\n",
    "print('Prediction:')\n",
    "print(tree)\n",
    "nltk.chunk.util.tree2conlltags(tree)\n",
    "regex_parser(ae_laptop_train['15']['sentence'])\n",
    "##\n",
    "\n",
    "gold_tree = pos_sent\n",
    "print('\\nGold Standard:')\n",
    "# create the tree with IOB input\n",
    "iob_gold_tree = nltk.Tree('S',[(el[0], el[1], ae_laptop_train['15']['label'][ind]) if ae_laptop_train['15']['label'][ind]=='O' \n",
    "                               else (el[0], el[1], ae_laptop_train['15']['label'][ind] + '-NP')for ind,el in enumerate(gold_tree)])\n",
    "print(nltk.chunk.util.conlltags2tree(iob_gold_tree))\n",
    "# print(nltk.chunk.util.conlltags2tree([(el[0], el[1], ae_laptop_train['15']['label'][ind])for ind,el in enumerate(gold_tree)]))\n",
    "# print(nltk.chunk.util.conlltags2tree())\n",
    "print(cp.evaluate([iob_gold_tree]))\n",
    "\n",
    "# nltk.chunk.util.tagstr2tree(' '.join(tokenized_sentence), chunk_label='NP', root_label='S', sep='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get ChunkScore to work on all dev\n",
    "print('\\nGold Standard:')\n",
    "# tag every sentence with the pos\n",
    "gold_tree = ae_laptop_dev_df['sentence'].apply(lambda x: nltk.pos_tag(x))\n",
    "print(gold_tree)\n",
    "iob_gold_tree = [nltk.Tree('S',\n",
    "                           [(el[0], el[1], ae_laptop_dev_df.iloc[tree_ind]['label'][ind])\n",
    "                            if ae_laptop_dev_df.iloc[tree_ind]['label'][ind]=='O'\n",
    "                            else (el[0], el[1], ae_laptop_dev_df.iloc[tree_ind]['label'][ind] + '-NP')\n",
    "                            for ind,el in enumerate(tree)])\n",
    "                for tree_ind, tree in enumerate(gold_tree)]\n",
    "ae_laptop_dev_df['iob_gold_tree'] = iob_gold_tree\n",
    "ae_laptop_dev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AE evaluation - Token Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(true,predictions):\n",
    "    accuracy = []\n",
    "    for true_el, predict_el in zip(true,predictions):\n",
    "        accuracy.append((np.array(predict_el) == np.array(true_el)).sum() / (len(true_el)))\n",
    "    return(accuracy)\n",
    "\n",
    "ae_laptop_dev_df['accuracy'] = get_accuracy(ae_laptop_dev_df.label,ae_laptop_dev_df.predictions)\n",
    "ae_laptop_dev_df['accuracy_1'] = get_accuracy(ae_laptop_dev_df.label,ae_laptop_dev_df.predictions_1)\n",
    "ae_laptop_dev_df.head()\n",
    "ae_laptop_dev_df[['accuracy','accuracy_1']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export samples that are well / poorly extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_examples(n = 1):\n",
    "    ind = np.argpartition(ae_laptop_dev_df.accuracy + ae_laptop_dev_df.accuracy_1,n)[:n]\n",
    "    print(ind)\n",
    "    sort_ind = ind[np.argsort((ae_laptop_dev_df.accuracy + ae_laptop_dev_df.accuracy_1).iloc[ind])]\n",
    "    bad_example = ae_laptop_dev_df.iloc[sort_ind]\n",
    "    \n",
    "    display(bad_example)\n",
    "    print(*[' '.join(sent) for sent in bad_example.sentence],sep='\\n')\n",
    "    \n",
    "get_bad_examples(3)\n",
    "\n",
    "def get_good_examples(n = 1):\n",
    "    ind = np.argpartition(ae_laptop_dev_df.accuracy + ae_laptop_dev_df.accuracy_1,-n)[-n:]\n",
    "    print(ind)\n",
    "    sort_ind = ind[np.argsort((ae_laptop_dev_df.accuracy + ae_laptop_dev_df.accuracy_1).iloc[ind])]\n",
    "    good_example = ae_laptop_dev_df.iloc[sort_ind]\n",
    "    \n",
    "    display(good_example)\n",
    "    print(*[' '.join(sent) for sent in good_example.sentence],sep='\\n')\n",
    "# good_example = ae_laptop_dev_df.iloc[np.argmax(ae_laptop_dev_df.accuracy + ae_laptop_dev_df.accuracy_1)]\n",
    "get_good_examples(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with ASC baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not even safe mode boots.---------------------------------------- -0.3412 (negative)\n",
      "Keyboard was also very nice and had a solid feel.---------------- 0.5709 (positive)\n",
      "Keyboard is plastic and spongey feeling.------------------------- 0.128 (positive)\n",
      "I would recommend this laptop to anyone looking to get a new laptop who is willing to spend a little more money to get great quality! 0.784 (positive)\n",
      "Thus, when you carry it at a slanted angle, the screen will \"topple\" or \"slide\" down, if you understand what I mean. 0.0 (neutral)\n",
      "When I called Sony the Customer Service was Great.--------------- 0.6249 (positive)\n",
      "I also did not like the loud noises it made or how the bottom of the computer would get really hot. -0.2755 (negative)\n",
      "I also did not like the loud noises it made or how the bottom of the computer would get really hot. -0.2755 (negative)\n",
      "Also, one of the users mentioned how the edges on the macbook is sharp, if you have money to spend on one of the incase shells, it doesn't seem to be a problem. -0.4019 (negative)\n",
      "Also, one of the users mentioned how the edges on the macbook is sharp, if you have money to spend on one of the incase shells, it doesn't seem to be a problem. -0.4019 (negative)\n"
     ]
    }
   ],
   "source": [
    "def vader_asc(sentence_lst):\n",
    "    \"\"\"\n",
    "    For every sentence in the list, tag it as a positive/negative sentiment based on the sum of the words.\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    pos_neg_tag_lst = []\n",
    "    for ind,sentence in enumerate(sentence_lst):\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        pos_neg_tag = 'negative' if vs['compound'] <= -0.05 else 'positive' if vs['compound'] >= 0.05 else 'neutral' \n",
    "        # print first 10 examples\n",
    "        if ind <10: print(\"{:-<65} {} ({})\".format(sentence, str(vs['compound']),pos_neg_tag))\n",
    "        pos_neg_tag_lst.append(pos_neg_tag)\n",
    "    return(pos_neg_tag_lst)\n",
    "\n",
    "asc_laptop_dev_df['predictions'] = vader_asc(asc_laptop_dev_df.sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore ASC evaluation - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asc_laptop_dev_df.head()\n",
    "(asc_laptop_dev_df.polarity == asc_laptop_dev_df.predictions).value_counts(normalize=True)\n",
    "accuracy_score(asc_laptop_dev_df.polarity,asc_laptop_dev_df.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore ASC evaluation - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(asc_laptop_dev_df.polarity,asc_laptop_dev_df.predictions,labels=['negative','neutral','positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore ASC evaluation - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(asc_laptop_dev_df.polarity,asc_laptop_dev_df.predictions,labels=['negative','neutral','positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ASC evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asc_eval(y_true, y_pred):\n",
    "    print(confusion_matrix(y_true,y_pred,labels=['negative','neutral','positive']))\n",
    "    print(classification_report(y_true,asc_laptop_dev_df.predictions,labels=['negative','neutral','positive']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
