{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data\n",
    "**Author:** Jane Hung  \n",
    "**Date:** 1 Mar 2020  \n",
    "**Citations:**  \n",
    "@inproceedings{xu_bert2019,\n",
    "    title = \"BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis\",\n",
    "    author = \"Xu, Hu and Liu, Bing and Shu, Lei and Yu, Philip S.\",\n",
    "    booktitle = \"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics\",\n",
    "    year = \"2019\",\n",
    "}  \n",
    "https://drive.google.com/file/d/1NGH5bqzEx6aDlYJ7O3hepZF4i_p4iMR8/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "\n",
    "import matplotlib as mpl\n",
    "if os.environ.get('DISPLAY','') == '':\n",
    "    print('no display found. Using non-interactive Agg backend')\n",
    "    mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    f = open(filename,'r')\n",
    "    data = json.loads(f.read())\n",
    "    print('\\n',filename)\n",
    "    pprint.pprint(dict(list(data.items())[:1]))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ../data/hu-data/ae/laptop/train.json\n",
      "{'0': {'label': ['B',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'B',\n",
      "                 'I',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O'],\n",
      "       'sentence': ['Keyboard',\n",
      "                    'is',\n",
      "                    'great',\n",
      "                    'but',\n",
      "                    'primary',\n",
      "                    'and',\n",
      "                    'secondary',\n",
      "                    'control',\n",
      "                    'buttons',\n",
      "                    'could',\n",
      "                    'be',\n",
      "                    'more',\n",
      "                    'durable',\n",
      "                    '.']}}\n",
      "\n",
      " ../data/hu-data/ae/rest/train.json\n",
      "{'0': {'label': ['O', 'O', 'O', 'B'],\n",
      "       'sentence': ['I', 'LOVE', 'their', 'Thai']}}\n",
      "\n",
      " ../data/hu-data/asc/laptop/train.json\n",
      "{'327_0': {'id': '327_0',\n",
      "           'polarity': 'positive',\n",
      "           'sentence': 'Also it is very good for college students who just '\n",
      "                       'need a reliable, easy to use computer.',\n",
      "           'term': 'use'}}\n",
      "\n",
      " ../data/hu-data/asc/rest/train.json\n",
      "{'1592_0': {'id': '1592_0',\n",
      "            'polarity': 'positive',\n",
      "            'sentence': 'Our server was very helpful and friendly.',\n",
      "            'term': 'server'}}\n"
     ]
    }
   ],
   "source": [
    "ae_laptop_train = read_json('../data/hu-data/ae/laptop/train.json')\n",
    "ae_rest_train = read_json('../data/hu-data/ae/rest/train.json')\n",
    "\n",
    "\n",
    "asc_laptop_train = read_json('../data/hu-data/asc/laptop/train.json')\n",
    "asc_rest_train = read_json('../data/hu-data/asc/rest/train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ../data/hu-data/ae/laptop/dev.json\n",
      "{'0': {'label': ['O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O'],\n",
      "       'sentence': ['I',\n",
      "                    'have',\n",
      "                    'had',\n",
      "                    'this',\n",
      "                    'laptop',\n",
      "                    'for',\n",
      "                    'a',\n",
      "                    'few',\n",
      "                    'months',\n",
      "                    'now',\n",
      "                    'and',\n",
      "                    'i',\n",
      "                    'would',\n",
      "                    'say',\n",
      "                    'im',\n",
      "                    'pretty',\n",
      "                    'satisfied',\n",
      "                    '.']}}\n",
      "\n",
      " ../data/hu-data/ae/rest/dev.json\n",
      "{'0': {'label': ['O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'B',\n",
      "                 'I',\n",
      "                 'I',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O',\n",
      "                 'O'],\n",
      "       'sentence': ['In',\n",
      "                    'the',\n",
      "                    'summer',\n",
      "                    'months',\n",
      "                    ',',\n",
      "                    'the',\n",
      "                    'back',\n",
      "                    'garden',\n",
      "                    'area',\n",
      "                    'is',\n",
      "                    'really',\n",
      "                    'nice',\n",
      "                    '.']}}\n",
      "\n",
      " ../data/hu-data/asc/laptop/dev.json\n",
      "{'1113_0': {'id': '1113_0',\n",
      "            'polarity': 'negative',\n",
      "            'sentence': 'Not even safe mode boots.',\n",
      "            'term': 'safe mode'}}\n",
      "\n",
      " ../data/hu-data/asc/rest/dev.json\n",
      "{'2516_0': {'id': '2516_0',\n",
      "            'polarity': 'positive',\n",
      "            'sentence': \"This is the only Thai place I go too in NYC, it's \"\n",
      "                        'wonderful, and live relaxed Jazz on certain nights.',\n",
      "            'term': 'Jazz'}}\n"
     ]
    }
   ],
   "source": [
    "ae_laptop_dev  = read_json('../data/hu-data/ae/laptop/dev.json')\n",
    "ae_rest_dev = read_json('../data/hu-data/ae/rest/dev.json')\n",
    "\n",
    "\n",
    "asc_laptop_dev = read_json('../data/hu-data/asc/laptop/dev.json')\n",
    "asc_rest_dev = read_json('../data/hu-data/asc/rest/dev.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How do we get from the ASC data back to the AE data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polarity': 'positive',\n",
       " 'term': 'use',\n",
       " 'id': '327_0',\n",
       " 'sentence': 'Also it is very good for college students who just need a reliable, easy to use computer.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'label': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " 'sentence': ['If',\n",
       "  'you',\n",
       "  'could',\n",
       "  'stretch',\n",
       "  'by',\n",
       "  'a',\n",
       "  'few',\n",
       "  '100',\n",
       "  'dollars',\n",
       "  'I',\n",
       "  'highly',\n",
       "  'recommend',\n",
       "  'you',\n",
       "  'should',\n",
       "  'replace',\n",
       "  'your',\n",
       "  'Windows',\n",
       "  'laptop',\n",
       "  'with',\n",
       "  'this',\n",
       "  'one',\n",
       "  '.']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_laptop_train['327_0']\n",
    "ae_laptop_train['400']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Also',\n",
       " 'it',\n",
       " 'is',\n",
       " 'very',\n",
       " 'good',\n",
       " 'for',\n",
       " 'college',\n",
       " 'students',\n",
       " 'who',\n",
       " 'just',\n",
       " 'need',\n",
       " 'a',\n",
       " 'reliable',\n",
       " ',',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'use',\n",
       " 'computer',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(asc_laptop_train['327_0']['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with AE baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Keyboard', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('great', 'ADJ'),\n",
       " ('but', 'CONJ'),\n",
       " ('primary', 'ADJ'),\n",
       " ('and', 'CONJ'),\n",
       " ('secondary', 'ADJ'),\n",
       " ('control', 'NOUN'),\n",
       " ('buttons', 'NOUN'),\n",
       " ('could', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('more', 'ADV'),\n",
       " ('durable', 'ADJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag with universal POS. Especially for nouns\n",
    "nltk.pos_tag(ae_laptop_train['0']['sentence'],tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[I, have, had, this, laptop, for, a, few, mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, B, I, O, O, O, O, O, O, B, O, O, ...</td>\n",
       "      <td>[Additional, caveat, :, the, base, installatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, B, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "      <td>[it, is, of, high, quality, ,, has, a, killer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, B, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[The, screen, gets, smeary, and, dusty, very, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[I, previously, owned, an, HP, desktop, and, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "      <td>[The, benefits, were, immediate, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[All-, in-, all, ,, I, would, definitely, reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "      <td>[just, chill, and, enjoy, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[My, son, and, his, family, have, a, hard, tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[This, is, what, they, told, me, :, It, heats,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 label  \\\n",
       "0    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1    [O, O, O, O, B, I, O, O, O, O, O, O, B, O, O, ...   \n",
       "2    [O, O, O, O, B, O, O, O, O, B, O, O, O, O, O, ...   \n",
       "3           [O, B, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                    [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "..                                                 ...   \n",
       "145                                    [O, O, O, O, O]   \n",
       "146         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "147                                    [O, O, O, O, O]   \n",
       "148  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "149  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                              sentence  \n",
       "0    [I, have, had, this, laptop, for, a, few, mont...  \n",
       "1    [Additional, caveat, :, the, base, installatio...  \n",
       "2    [it, is, of, high, quality, ,, has, a, killer,...  \n",
       "3    [The, screen, gets, smeary, and, dusty, very, ...  \n",
       "4    [I, previously, owned, an, HP, desktop, and, a...  \n",
       "..                                                 ...  \n",
       "145                [The, benefits, were, immediate, !]  \n",
       "146  [All-, in-, all, ,, I, would, definitely, reco...  \n",
       "147                       [just, chill, and, enjoy, .]  \n",
       "148  [My, son, and, his, family, have, a, hard, tim...  \n",
       "149  [This, is, what, they, told, me, :, It, heats,...  \n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_laptop_dev_df = pd.DataFrame.from_dict(ae_laptop_dev,orient='index')\n",
    "ae_laptop_dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[I, have, had, this, laptop, for, a, few, mont...</td>\n",
       "      <td>[O, O, O, O, B, O, O, O, B, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, B, I, O, O, O, O, O, O, B, O, O, ...</td>\n",
       "      <td>[Additional, caveat, :, the, base, installatio...</td>\n",
       "      <td>[O, B, O, O, B, I, O, O, O, B, O, O, B, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, B, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "      <td>[it, is, of, high, quality, ,, has, a, killer,...</td>\n",
       "      <td>[O, O, O, O, B, O, O, O, B, I, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, B, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[The, screen, gets, smeary, and, dusty, very, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[I, previously, owned, an, HP, desktop, and, a...</td>\n",
       "      <td>[O, O, O, O, B, I, O, O, B, I, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, B, I, O, O, O, O, O, O, B, O, O, ...   \n",
       "2  [O, O, O, O, B, O, O, O, O, B, O, O, O, O, O, ...   \n",
       "3         [O, B, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  [I, have, had, this, laptop, for, a, few, mont...   \n",
       "1  [Additional, caveat, :, the, base, installatio...   \n",
       "2  [it, is, of, high, quality, ,, has, a, killer,...   \n",
       "3  [The, screen, gets, smeary, and, dusty, very, ...   \n",
       "4  [I, previously, owned, an, HP, desktop, and, a...   \n",
       "\n",
       "                                         predictions  \n",
       "0  [O, O, O, O, B, O, O, O, B, O, O, O, O, O, O, ...  \n",
       "1  [O, B, O, O, B, I, O, O, O, B, O, O, B, O, O, ...  \n",
       "2  [O, O, O, O, B, O, O, O, B, I, O, O, O, O, O, ...  \n",
       "3         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4                  [O, O, O, O, B, I, O, O, B, I, O]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0      [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "1      [0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...\n",
       "2      [0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, ...\n",
       "3             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "4                      [0, 0, 0, 0, 1, 2, 0, 0, 1, 2, 0]\n",
       "                             ...                        \n",
       "145                                      [0, 1, 0, 0, 0]\n",
       "146           [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
       "147                                      [0, 1, 0, 1, 0]\n",
       "148    [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...\n",
       "149    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: predictions, Length: 150, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_ae(tokenized_sentence):\n",
    "    \"\"\"\n",
    "    Tag sentences using POS tagger\n",
    "    \"\"\"\n",
    "    pos_sent = tokenized_sentence.apply(lambda sent:nltk.pos_tag(sent,tagset='universal'))\n",
    "    \n",
    "    \n",
    "    # tag with BIO terminology\n",
    "    ae_tag = lambda sent:['O' if token[1] != 'NOUN' \n",
    "                          else 'B' if ((token[1]=='NOUN') & (sent[ind-1][1]!='NOUN')) \n",
    "                          else 'I' for ind,token in enumerate(sent)]\n",
    "\n",
    "    return(pos_sent.apply(ae_tag))\n",
    "\n",
    "# since the POS tagger is based on the words themselves and not context.\n",
    "ae_laptop_dev_df['predictions'] = pos_ae(ae_laptop_dev_df['sentence'])\n",
    "ae_laptop_dev_df.head()\n",
    "\n",
    "def convert_int(tagged_tokens):\n",
    "    \"\"\"\n",
    "    Convert B,I,O tags to integers\n",
    "    \"\"\"\n",
    "    return(tagged_tokens.apply(lambda sent: [0 if token=='O' else 1 if token=='B' else 2 for token in sent]))\n",
    "\n",
    "convert_int(ae_laptop_dev_df['predictions'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AE classifier-based NP Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try a more sophisticated method for chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " 'sentence': ['Toshiba',\n",
       "  'is',\n",
       "  'aware',\n",
       "  'of',\n",
       "  'the',\n",
       "  'issue',\n",
       "  'but',\n",
       "  'unless',\n",
       "  'the',\n",
       "  'extended',\n",
       "  'warrenty',\n",
       "  'is',\n",
       "  'bought',\n",
       "  'Toshiba',\n",
       "  'will',\n",
       "  'do',\n",
       "  'nothing',\n",
       "  'about',\n",
       "  'it',\n",
       "  '.']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('Toshiba', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('aware', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('issue', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('unless', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('extended', 'JJ'),\n",
       " ('warrenty', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('bought', 'VBN'),\n",
       " ('Toshiba', 'NNP'),\n",
       " ('will', 'MD'),\n",
       " ('do', 'VB'),\n",
       " ('nothing', 'NN'),\n",
       " ('about', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_laptop_train['15']\n",
    "\n",
    "sentence = nltk.pos_tag(ae_laptop_train['15']['sentence'])\n",
    "sentence\n",
    "\n",
    "def regex_parser(tokenized_sentence):\n",
    "    grammar = r\"\"\"\n",
    "      NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "          {<NNP>+}                # chunk sequences of proper nouns\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "    result = cp.parse(tokenized_sentence)\n",
    "    return(result)\n",
    "tree = regex_parser(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Toshiba/NNP)\n",
      "  is/VBZ\n",
      "  aware/JJ\n",
      "  of/IN\n",
      "  (NP the/DT issue/NN)\n",
      "  but/CC\n",
      "  unless/IN\n",
      "  (NP the/DT extended/JJ warrenty/NN)\n",
      "  is/VBZ\n",
      "  bought/VBN\n",
      "  (NP Toshiba/NNP)\n",
      "  will/MD\n",
      "  do/VB\n",
      "  (NP nothing/NN)\n",
      "  about/IN\n",
      "  it/PRP\n",
      "  ./.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, Tree('NP', [('Toshiba', 'NNP')])),\n",
       " (1, ('is', 'VBZ')),\n",
       " (2, ('aware', 'JJ')),\n",
       " (3, ('of', 'IN')),\n",
       " (4, Tree('NP', [('the', 'DT'), ('issue', 'NN')])),\n",
       " (5, ('but', 'CC')),\n",
       " (6, ('unless', 'IN')),\n",
       " (7, Tree('NP', [('the', 'DT'), ('extended', 'JJ'), ('warrenty', 'NN')])),\n",
       " (8, ('is', 'VBZ')),\n",
       " (9, ('bought', 'VBN')),\n",
       " (10, Tree('NP', [('Toshiba', 'NNP')])),\n",
       " (11, ('will', 'MD')),\n",
       " (12, ('do', 'VB')),\n",
       " (13, Tree('NP', [('nothing', 'NN')])),\n",
       " (14, ('about', 'IN')),\n",
       " (15, ('it', 'PRP')),\n",
       " (16, ('.', '.'))]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(Tree('NP', [('Toshiba', 'NNP')]), 'O'),\n",
       " (('Toshiba', 'NNP'), 'B'),\n",
       " (('the', 'DT'), 'B'),\n",
       " (('the', 'DT'), 'B'),\n",
       " (('Toshiba', 'NNP'), 'B'),\n",
       " (('nothing', 'NN'), 'B')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tree)\n",
    "[(ind, node) for ind, node in enumerate(tree)]\n",
    "[(subtree[0],'B') if 'NP' in subtree.label() else (subtree[0],'O')for subtree in tree.subtrees() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [Tree('NP', [('Rockwell', 'NNP'), ('International', 'NNP'), ('Corp.', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('Tulsa', 'NNP'), ('unit', 'NN')]), ('said', 'VBD'), Tree('NP', [('it', 'PRP')]), ('signed', 'VBD'), Tree('NP', [('a', 'DT'), ('tentative', 'JJ'), ('agreement', 'NN')]), ('extending', 'VBG'), Tree('NP', [('its', 'PRP$'), ('contract', 'NN')]), ('with', 'IN'), Tree('NP', [('Boeing', 'NNP'), ('Co.', 'NNP')]), ('to', 'TO'), ('provide', 'VB'), Tree('NP', [('structural', 'JJ'), ('parts', 'NNS')]), ('for', 'IN'), Tree('NP', [('Boeing', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('747', 'CD'), ('jetliners', 'NNS')]), ('.', '.')]), Tree('S', [Tree('NP', [('Rockwell', 'NNP')]), ('said', 'VBD'), Tree('NP', [('the', 'DT'), ('agreement', 'NN')]), ('calls', 'VBZ'), ('for', 'IN'), Tree('NP', [('it', 'PRP')]), ('to', 'TO'), ('supply', 'VB'), Tree('NP', [('200', 'CD'), ('additional', 'JJ'), ('so-called', 'JJ'), ('shipsets', 'NNS')]), ('for', 'IN'), Tree('NP', [('the', 'DT'), ('planes', 'NNS')]), ('.', '.')]), ...]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  67.7%%\n",
      "    Precision:     48.1%%\n",
      "    Recall:        37.4%%\n",
      "    F-Measure:     42.1%%\n",
      "[Tree('S', [Tree('NP', [('Toshiba', 'NNP')]), ('is', 'VBZ'), ('aware', 'JJ'), ('of', 'IN'), Tree('NP', [('the', 'DT'), ('issue', 'NN')]), ('but', 'CC'), ('unless', 'IN'), Tree('NP', [('the', 'DT'), ('extended', 'JJ'), ('warrenty', 'NN')]), ('is', 'VBZ'), ('bought', 'VBN'), Tree('NP', [('Toshiba', 'NNP')]), ('will', 'MD'), ('do', 'VB'), Tree('NP', [('nothing', 'NN')]), ('about', 'IN'), ('it', 'PRP'), ('.', '.')])]\n",
      "ChunkParse score:\n",
      "    IOB Accuracy: 100.0%%\n",
      "    Precision:    100.0%%\n",
      "    Recall:       100.0%%\n",
      "    F-Measure:    100.0%%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Toshiba NNP B-NP\\nis VBZ O\\naware JJ O\\nof IN O\\nthe DT B-NP\\nissue NN I-NP\\nbut CC O\\nunless IN O\\nthe DT B-NP\\nextended JJ I-NP\\nwarrenty NN I-NP\\nis VBZ O\\nbought VBN O\\nToshiba NNP B-NP\\nwill MD O\\ndo VB O\\nnothing NN B-NP\\nabout IN O\\nit PRP O\\n. . O'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'I',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.evaluate(conll2000.chunked_sents('test.txt', chunk_types=['NP'])))\n",
    "print([tree])\n",
    "print(cp.evaluate([tree]))\n",
    "nltk.chunk.util.tree2conllstr(tree)\n",
    "[el[2][0] for el in nltk.chunk.util.tree2conlltags(tree)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AE evaluation - CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.837730665815654"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only using 0,1 because there aren't many very large token phrases\n",
    "log_loss(convert_int(pd.DataFrame(ae_laptop_dev_df.iloc[0]['label'])),convert_int(pd.DataFrame(ae_laptop_dev_df.iloc[0]['predictions'])),labels=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AE evaluation - SemEval14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to explore how we want to move forward with all sentences rather than just 1.\n",
    "# Should try to implement the SemEval14 evaluation criteria bc this is best practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with ASC baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>term</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1113_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>safe mode</td>\n",
       "      <td>1113_0</td>\n",
       "      <td>Not even safe mode boots.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595_0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>2595_0</td>\n",
       "      <td>Keyboard was also very nice and had a solid feel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>1039_0</td>\n",
       "      <td>Keyboard is plastic and spongey feeling.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315_0</th>\n",
       "      <td>positive</td>\n",
       "      <td>quality</td>\n",
       "      <td>315_0</td>\n",
       "      <td>I would recommend this laptop to anyone lookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>screen</td>\n",
       "      <td>1284_0</td>\n",
       "      <td>Thus, when you carry it at a slanted angle, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity       term      id  \\\n",
       "1113_0  negative  safe mode  1113_0   \n",
       "2595_0  positive   Keyboard  2595_0   \n",
       "1039_0  negative   Keyboard  1039_0   \n",
       "315_0   positive    quality   315_0   \n",
       "1284_0  negative     screen  1284_0   \n",
       "\n",
       "                                                 sentence  \n",
       "1113_0                          Not even safe mode boots.  \n",
       "2595_0  Keyboard was also very nice and had a solid feel.  \n",
       "1039_0           Keyboard is plastic and spongey feeling.  \n",
       "315_0   I would recommend this laptop to anyone lookin...  \n",
       "1284_0  Thus, when you carry it at a slanted angle, th...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_laptop_dev_df = pd.DataFrame.from_dict(asc_laptop_dev,orient='index')\n",
    "asc_laptop_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not even safe mode boots.---------------------------------------- -0.3412 (negative)\n",
      "Keyboard was also very nice and had a solid feel.---------------- 0.5709 (positive)\n",
      "Keyboard is plastic and spongey feeling.------------------------- 0.128 (positive)\n",
      "I would recommend this laptop to anyone looking to get a new laptop who is willing to spend a little more money to get great quality! 0.784 (positive)\n",
      "Thus, when you carry it at a slanted angle, the screen will \"topple\" or \"slide\" down, if you understand what I mean. 0.0 (neutral)\n",
      "When I called Sony the Customer Service was Great.--------------- 0.6249 (positive)\n",
      "I also did not like the loud noises it made or how the bottom of the computer would get really hot. -0.2755 (negative)\n",
      "I also did not like the loud noises it made or how the bottom of the computer would get really hot. -0.2755 (negative)\n",
      "Also, one of the users mentioned how the edges on the macbook is sharp, if you have money to spend on one of the incase shells, it doesn't seem to be a problem. -0.4019 (negative)\n",
      "Also, one of the users mentioned how the edges on the macbook is sharp, if you have money to spend on one of the incase shells, it doesn't seem to be a problem. -0.4019 (negative)\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "pos_neg_tag_lst = []\n",
    "for ind,sentence in enumerate(asc_laptop_dev_df.sentence):\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    pos_neg_tag = 'negative' if vs['compound'] <= -0.05 else 'positive' if vs['compound'] >= 0.05 else 'neutral' \n",
    "    if ind <10: print(\"{:-<65} {} ({})\".format(sentence, str(vs['compound']),pos_neg_tag))\n",
    "    pos_neg_tag_lst.append(pos_neg_tag)\n",
    "asc_laptop_dev_df['predictions'] = pos_neg_tag_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore ASC evaluation - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>term</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1113_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>safe mode</td>\n",
       "      <td>1113_0</td>\n",
       "      <td>Not even safe mode boots.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595_0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>2595_0</td>\n",
       "      <td>Keyboard was also very nice and had a solid feel.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>1039_0</td>\n",
       "      <td>Keyboard is plastic and spongey feeling.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315_0</th>\n",
       "      <td>positive</td>\n",
       "      <td>quality</td>\n",
       "      <td>315_0</td>\n",
       "      <td>I would recommend this laptop to anyone lookin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284_0</th>\n",
       "      <td>negative</td>\n",
       "      <td>screen</td>\n",
       "      <td>1284_0</td>\n",
       "      <td>Thus, when you carry it at a slanted angle, th...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity       term      id  \\\n",
       "1113_0  negative  safe mode  1113_0   \n",
       "2595_0  positive   Keyboard  2595_0   \n",
       "1039_0  negative   Keyboard  1039_0   \n",
       "315_0   positive    quality   315_0   \n",
       "1284_0  negative     screen  1284_0   \n",
       "\n",
       "                                                 sentence predictions  \n",
       "1113_0                          Not even safe mode boots.    negative  \n",
       "2595_0  Keyboard was also very nice and had a solid feel.    positive  \n",
       "1039_0           Keyboard is plastic and spongey feeling.    positive  \n",
       "315_0   I would recommend this laptop to anyone lookin...    positive  \n",
       "1284_0  Thus, when you carry it at a slanted angle, th...     neutral  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True     0.613333\n",
       "False    0.386667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_laptop_dev_df.head()\n",
    "(asc_laptop_dev_df.polarity == asc_laptop_dev_df.predictions).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'negative'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ee3d989dfd44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masc_laptop_dev_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0masc_laptop_dev_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2184\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mlogarithm\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnatural\u001b[0m \u001b[0mlogarithm\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m     \"\"\"\n\u001b[0;32m-> 2186\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2187\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    752\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'negative'"
     ]
    }
   ],
   "source": [
    "log_loss(asc_laptop_dev_df.polarity,asc_laptop_dev_df.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
